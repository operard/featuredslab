{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b72a8f6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th></tr><tr><td>21</td><td>application_1624276539905_0001</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"/hopsworks-api/yarnui/https://resourcemanager.service.consul:8089/proxy/application_1624276539905_0001/\">Link</a></td><td><a target=\"_blank\" href=\"/hopsworks-api/yarnui/https://featureds11:8044/node/containerlogs/container_e07_1624276539905_0001_01_000001/testcaixa2__meb10000\">Link</a></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n",
      "Connected. Call `.close()` to terminate connection gracefully."
     ]
    }
   ],
   "source": [
    "import hsfs\n",
    "# Create a connection\n",
    "connection = hsfs.connection()\n",
    "# Get the feature store handle for the project's feature store\n",
    "fs = connection.get_feature_store()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "01d8cdcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hops import hdfs\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# hdfs:///Projects/testholuser20/testholuser20_Training_Datasets/teams_features.csv\n",
    "teams_csv = spark.read\\\n",
    "             .option(\"inferSchema\", \"true\")\\\n",
    "             .option(\"header\", \"true\")\\\n",
    "             .format(\"csv\")\\\n",
    "             .load(\"hdfs:///Projects/{}/testcaixa2_Training_Datasets//teams_features.csv\".format(hdfs.project_name()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8673576c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50"
     ]
    }
   ],
   "source": [
    "teams_csv.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "10db7623",
   "metadata": {},
   "outputs": [],
   "source": [
    "online_teams_fg_meta = fs.create_feature_group(name=\"teams_features\",\n",
    "                                       version=1,\n",
    "                                       primary_key=['team_id'],\n",
    "                                       description=\"Store related features\",\n",
    "                                       online_enabled=True,\n",
    "                                       time_travel_format=None,\n",
    "                                       statistics_config={\"enabled\": True, \"histograms\": True, \"correlations\": True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "53c49c67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<hsfs.feature_group.FeatureGroup object at 0x7fad39a6d950>"
     ]
    }
   ],
   "source": [
    "online_teams_fg_meta.save(teams_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0e5a5467",
   "metadata": {},
   "outputs": [],
   "source": [
    "country_csv = spark.read\\\n",
    "             .option(\"inferSchema\", \"true\")\\\n",
    "             .option(\"header\", \"true\")\\\n",
    "             .format(\"csv\")\\\n",
    "             .option(\"delimiter\",\";\")\\\n",
    "             .load(\"hdfs:///Projects/{}/testcaixa2_Training_Datasets//data_cleaned_train.csv\".format(hdfs.project_name()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fa62db04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10592"
     ]
    }
   ],
   "source": [
    "country_csv.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4e9f6658",
   "metadata": {},
   "outputs": [],
   "source": [
    "online_name_country_fg_meta = fs.create_feature_group(name=\"name_country_fg\",\n",
    "                                       version=1,\n",
    "                                       primary_key=['first_name', 'last_name'],\n",
    "                                       description=\"Name - Country prediction\",\n",
    "                                       online_enabled=True,\n",
    "                                       time_travel_format=None,\n",
    "                                       statistics_config={\"enabled\": True, \"histograms\": True, \"correlations\": True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8c371d20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<hsfs.feature_group.FeatureGroup object at 0x7fad39a635d0>"
     ]
    }
   ],
   "source": [
    "online_name_country_fg_meta.save(country_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc71e4b",
   "metadata": {},
   "source": [
    "# Get"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7e70fb61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------+-------------+\n",
      "|team_budget|team_id|team_position|\n",
      "+-----------+-------+-------------+\n",
      "|  12957.076|      1|            1|\n",
      "|  2403.3704|      2|            2|\n",
      "|  3390.3755|      3|            3|\n",
      "|  13547.429|      4|            4|\n",
      "|   9678.333|      5|            5|\n",
      "+-----------+-------+-------------+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "teams_features = fs.get_feature_group(\"teams_features\",version=1)\n",
    "teams_features.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a30cbe9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: name_country_fg\n",
      "Description: Name - Country prediction\n",
      "Features:"
     ]
    }
   ],
   "source": [
    "print(\"Name: {}\".format(name_country_fg.name))\n",
    "print(\"Description: {}\".format(name_country_fg.description))\n",
    "print(\"Features:\")\n",
    "features = name_country_fg.features\n",
    "for feature in features:\n",
    "    print(\"{:<60} \\t Primary: {} \\t Partition: {}\".format(feature.name, feature.primary, feature.partition))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfcbcdd9",
   "metadata": {},
   "source": [
    "# Feature Data Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3d9bb600",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'HAS_MIN', 'predicate': 'VALUE', 'acceptedType': 'Fractional', 'description': 'Assert on the min of a feature.'}\n",
      "{'name': 'HAS_MAX', 'predicate': 'VALUE', 'acceptedType': 'Fractional', 'description': 'Assert on the max of a feature.'}\n",
      "{'name': 'HAS_COMPLETENESS', 'predicate': 'VALUE', 'acceptedType': 'Fractional', 'description': 'Assert on the uniqueness of a single or combined set of features.'}\n",
      "{'name': 'HAS_UNIQUE_VALUE_RATIO', 'predicate': 'VALUE', 'acceptedType': 'Fractional', 'description': 'Assert on the unique value ratio of of a single or combined set of features.'}\n",
      "{'name': 'HAS_NUMBER_OF_DISTINCT_VALUES', 'predicate': 'VALUE', 'acceptedType': 'Integral', 'description': 'Assert on the number of distinct values of a feature.'}\n",
      "{'name': 'HAS_ENTROPY', 'predicate': 'VALUE', 'acceptedType': 'Fractional', 'description': 'Assert on the entropy of a feature.'}\n",
      "{'name': 'HAS_MUTUAL_INFORMATION', 'predicate': 'LEGAL_VALUES', 'acceptedType': 'Fractional', 'description': 'Assert on the mutual information between two features.'}\n",
      "{'name': 'HAS_PATTERN', 'predicate': 'PATTERN', 'acceptedType': 'String', 'description': 'Assert on the average compliance of the feature to the regular expression.'}\n",
      "{'name': 'IS_NON_NEGATIVE', 'predicate': 'VALUE', 'acceptedType': 'Fractional', 'description': 'Assert on feature containing non negative values.'}\n",
      "{'name': 'IS_POSITIVE', 'predicate': 'VALUE', 'acceptedType': 'Boolean', 'description': 'Assert on a feature containing non negative values.'}\n",
      "{'name': 'IS_GREATER_THAN', 'predicate': 'LEGAL_VALUES', 'acceptedType': 'Fractional', 'description': 'Assert on feature A values being greater than feature B.'}\n",
      "{'name': 'IS_CONTAINED_IN', 'predicate': 'LEGAL_VALUES', 'acceptedType': 'String', 'description': 'Assert that every non-null value of feature is contained in a set of predefined values.'}\n",
      "{'name': 'HAS_MEAN', 'predicate': 'VALUE', 'acceptedType': 'Fractional', 'description': 'Assert on the mean of a feature.'}\n",
      "{'name': 'HAS_SUM', 'predicate': 'VALUE', 'acceptedType': 'Fractional', 'description': 'Assert on the sum of a feature.'}\n",
      "{'name': 'HAS_SIZE', 'predicate': 'VALUE', 'acceptedType': 'Integral', 'description': 'Assert on the number of rows of the dataframe.'}\n",
      "{'name': 'HAS_UNIQUENESS', 'predicate': 'VALUE', 'acceptedType': 'Fractional', 'description': 'Assert on the uniqueness of a single or combined set of features.'}\n",
      "{'name': 'HAS_DISTINCTNESS', 'predicate': 'VALUE', 'acceptedType': 'Fractional', 'description': 'Assert on the distincness of a single or combined set of features.'}\n",
      "{'name': 'HAS_APPROX_QUANTILE', 'predicate': 'VALUE', 'acceptedType': 'Fractional', 'description': 'Assert on the approximate quantile of a feature.'}\n",
      "{'name': 'HAS_STANDARD_DEVIATION', 'predicate': 'VALUE', 'acceptedType': 'Fractional', 'description': 'Assert on the standard deviation of a feature.'}\n",
      "{'name': 'HAS_APPROX_COUNT_DISTINCT', 'predicate': 'VALUE', 'acceptedType': 'Fractional', 'description': 'Assert on the approximate count distinct of a feature.'}\n",
      "{'name': 'HAS_CORRELATION', 'predicate': 'LEGAL_VALUES', 'acceptedType': 'Fractional', 'description': 'Assert on the pearson correlcation between two features.'}\n",
      "{'name': 'HAS_DATATYPE', 'predicate': 'ACCEPTED_TYPE', 'acceptedType': 'String', 'description': 'Assert on the fraction of rows that conform to the given data type.'}\n",
      "{'name': 'IS_LESS_THAN', 'predicate': 'LEGAL_VALUES', 'acceptedType': 'Fractional', 'description': 'Assert on feature A values being less that feature B.'}\n",
      "{'name': 'IS_LESS_THAN_OR_EQUAL_TO', 'predicate': 'LEGAL_VALUES', 'acceptedType': 'Fractional', 'description': 'Assert on feature A values being less or equal to those of feature B.'}\n",
      "{'name': 'IS_GREATER_THAN_OR_EQUAL_TO', 'predicate': 'LEGAL_VALUES', 'acceptedType': 'Fractional', 'description': 'Assert on feature A values being greater than or equal to those of feature B.'}\n",
      "[None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]"
     ]
    }
   ],
   "source": [
    "from hsfs.rule import Rule\n",
    "rules = connection.get_rules()\n",
    "[print(rule.to_dict()) for rule in rules]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a8f6bfc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "expectation.rules[0].to_dict(){'name': 'HAS_NUMBER_OF_DISTINCT_VALUES', 'level': 'ERROR', 'min': 1, 'max': None, 'value': None, 'pattern': None, 'acceptedType': None, 'legalValues': None}\n",
      "ExpectationsApi.expectation.to_dict(){'name': 'countries', 'description': 'min and max number of countries', 'features': ['country'], 'rules': [<hsfs.rule.Rule object at 0x7fad3371a8d0>, <hsfs.rule.Rule object at 0x7fad3371a050>]}\n",
      "ExpectationsApi.expectation.rules[0].to_dict(){'name': 'HAS_NUMBER_OF_DISTINCT_VALUES', 'level': 'ERROR', 'min': 1, 'max': None, 'value': None, 'pattern': None, 'acceptedType': None, 'legalValues': None}\n",
      "ExpectationsApi.expectation.payload{\"name\": \"countries\", \"description\": \"min and max number of countries\", \"features\": [\"country\"], \"rules\": [{\"name\": \"HAS_NUMBER_OF_DISTINCT_VALUES\", \"level\": \"ERROR\", \"min\": 1, \"max\": null, \"value\": null, \"pattern\": null, \"acceptedType\": null, \"legalValues\": null}, {\"name\": \"HAS_NUMBER_OF_DISTINCT_VALUES\", \"level\": \"ERROR\", \"min\": null, \"max\": 195, \"value\": null, \"pattern\": null, \"acceptedType\": null, \"legalValues\": null}]}"
     ]
    }
   ],
   "source": [
    "expectation_countries = fs.create_expectation(\"countries\",\n",
    "                                          description=\"min and max number of countries\",\n",
    "                                          features=[\"country\"], \n",
    "                                          rules=[Rule(name=\"HAS_NUMBER_OF_DISTINCT_VALUES\", level=\"ERROR\", min=1), \n",
    "                                                 Rule(name=\"HAS_NUMBER_OF_DISTINCT_VALUES\", level=\"ERROR\", max=195)])\n",
    "expectation_countries.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "df810bec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "Metadata operation error: (url: https://hopsworks.glassfish.service.consul:8182/hopsworks-api/api/project/124/featurestores/68/featuregroups/None/expectations/countries). Server response: \n",
      "HTTP code: 404, HTTP reason: Not Found, error code: 120004, error msg: Web application exception occurred, user msg: HTTP 404 Not Found\n",
      "Traceback (most recent call last):\n",
      "  File \"/srv/hops/anaconda/envs/theenv/lib/python3.7/site-packages/hsfs/feature_group.py\", line 897, in attach_expectation\n",
      "    return self._expectations_api.attach(self, expectation.name)\n",
      "  File \"/srv/hops/anaconda/envs/theenv/lib/python3.7/site-packages/hsfs/core/expectations_api.py\", line 77, in attach\n",
      "    _client._send_request(\"PUT\", path_params)\n",
      "  File \"/srv/hops/anaconda/envs/theenv/lib/python3.7/site-packages/hsfs/decorators.py\", line 35, in if_connected\n",
      "    return fn(inst, *args, **kwargs)\n",
      "  File \"/srv/hops/anaconda/envs/theenv/lib/python3.7/site-packages/hsfs/client/base.py\", line 147, in _send_request\n",
      "    raise exceptions.RestAPIError(url, response)\n",
      "hsfs.client.exceptions.RestAPIError: Metadata operation error: (url: https://hopsworks.glassfish.service.consul:8182/hopsworks-api/api/project/124/featurestores/68/featuregroups/None/expectations/countries). Server response: \n",
      "HTTP code: 404, HTTP reason: Not Found, error code: 120004, error msg: Web application exception occurred, user msg: HTTP 404 Not Found\n",
      "\n"
     ]
    }
   ],
   "source": [
    "name_country_fg.attach_expectation(expectation_countries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "463a2ce7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "An error occurred while calling o371.save.\n",
      ": org.apache.hudi.exception.HoodieException: 'hoodie.table.name', 'path' must be set.\n",
      "\tat org.apache.hudi.HoodieSparkSqlWriter$.write(HoodieSparkSqlWriter.scala:75)\n",
      "\tat org.apache.hudi.DefaultSource.createRelation(DefaultSource.scala:134)\n",
      "\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:45)\n",
      "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)\n",
      "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)\n",
      "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:86)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:80)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:80)\n",
      "\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)\n",
      "\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:676)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:285)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:271)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/srv/hops/anaconda/envs/theenv/lib/python3.7/site-packages/hsfs/feature_group.py\", line 708, in insert\n",
      "    write_options,\n",
      "  File \"/srv/hops/anaconda/envs/theenv/lib/python3.7/site-packages/hsfs/core/feature_group_engine.py\", line 101, in insert\n",
      "    validation_id,\n",
      "  File \"/srv/hops/anaconda/envs/theenv/lib/python3.7/site-packages/hsfs/engine/spark.py\", line 218, in save_dataframe\n",
      "    offline_write_options,\n",
      "  File \"/srv/hops/anaconda/envs/theenv/lib/python3.7/site-packages/hsfs/engine/spark.py\", line 286, in _save_offline_dataframe\n",
      "    dataframe, self.APPEND, operation, write_options, validation_id\n",
      "  File \"/srv/hops/anaconda/envs/theenv/lib/python3.7/site-packages/hsfs/core/hudi_engine.py\", line 102, in save_hudi_fg\n",
      "    dataset, save_mode, operation, write_options\n",
      "  File \"/srv/hops/anaconda/envs/theenv/lib/python3.7/site-packages/hsfs/core/hudi_engine.py\", line 129, in _write_hudi_dataset\n",
      "    ).save(self._base_path)\n",
      "  File \"/srv/hops/spark/python/lib/pyspark.zip/pyspark/sql/readwriter.py\", line 732, in save\n",
      "    self._jwrite.save()\n",
      "  File \"/srv/hops/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1257, in __call__\n",
      "    answer, self.gateway_client, self.target_id, self.name)\n",
      "  File \"/srv/hops/spark/python/lib/pyspark.zip/pyspark/sql/utils.py\", line 63, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/srv/hops/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\", line 328, in get_return_value\n",
      "    format(target_id, \".\", name), value)\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling o371.save.\n",
      ": org.apache.hudi.exception.HoodieException: 'hoodie.table.name', 'path' must be set.\n",
      "\tat org.apache.hudi.HoodieSparkSqlWriter$.write(HoodieSparkSqlWriter.scala:75)\n",
      "\tat org.apache.hudi.DefaultSource.createRelation(DefaultSource.scala:134)\n",
      "\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:45)\n",
      "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)\n",
      "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)\n",
      "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:86)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:80)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:80)\n",
      "\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)\n",
      "\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:676)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:285)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:271)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a Pandas Dataframe and ingest its features into a feature group that you create here.  \n",
    "import pandas as pd \n",
    "columns = ['first_name', 'last_name', 'country']\n",
    "data = [['tom', 'johnson', 'UK'], ['penelope', 'charles', 'UK'], ['harry', 'windsor', \"USA\"]]   \n",
    "df = pd.DataFrame(data, columns=columns) \n",
    "name_country_fg.insert(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56aac704",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affaea09",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}