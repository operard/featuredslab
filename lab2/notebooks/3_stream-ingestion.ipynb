{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0dbcadd4",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Windowed aggregations using spark streaming and ingestion to the online feature store\"\n",
    "date: 2021-04-25\n",
    "type: technical_note\n",
    "draft: false\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96699c0d",
   "metadata": {},
   "source": [
    "## Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9fd3341",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th></tr><tr><td>22</td><td>application_1624276539905_0002</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"/hopsworks-api/yarnui/https://resourcemanager.service.consul:8089/proxy/application_1624276539905_0002/\">Link</a></td><td><a target=\"_blank\" href=\"/hopsworks-api/yarnui/https://featureds11:8044/node/containerlogs/container_e07_1624276539905_0002_01_000001/kafka__holuse19\">Link</a></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "from pyspark.sql.functions import from_json, window, avg,count, stddev, explode, date_format,col\n",
    "from pyspark.sql.types import StructField, StructType, StringType, DoubleType, TimestampType, LongType, IntegerType\n",
    "\n",
    "from hops import kafka, tls, hdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35b8be22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# name of the kafka topic to read card transactions from\n",
    "KAFKA_TOPIC_NAME = \"credit_card_transactions\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f701593f",
   "metadata": {},
   "source": [
    "## Create a stream from the kafka topic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "2c8fdcf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_read = spark \\\n",
    "  .readStream \\\n",
    "  .format(\"kafka\") \\\n",
    "  .option(\"kafka.bootstrap.servers\", kafka.get_broker_endpoints()) \\\n",
    "  .option(\"kafka.security.protocol\",kafka.get_security_protocol()) \\\n",
    "  .option(\"kafka.ssl.truststore.location\", tls.get_trust_store()) \\\n",
    "  .option(\"kafka.ssl.truststore.password\", tls.get_key_store_pwd()) \\\n",
    "  .option(\"kafka.ssl.keystore.location\", tls.get_key_store()) \\\n",
    "  .option(\"kafka.ssl.keystore.password\", tls.get_key_store_pwd()) \\\n",
    "  .option(\"kafka.ssl.key.password\", tls.get_trust_store_pwd()) \\\n",
    "  .option(\"kafka.ssl.endpoint.identification.algorithm\", \"\") \\\n",
    "  .option(\"startingOffsets\", \"earliest\")\\\n",
    "  .option(\"subscribe\", KAFKA_TOPIC_NAME) \\\n",
    "  .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "17dd2652",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define schema to read from kafka topic \n",
    "parse_schema = StructType([StructField('tid', StringType(), True),\n",
    "                           StructField('datetime', StringType(), True),\n",
    "                           StructField('cc_num', StringType(), True),\n",
    "                           StructField('amount', StringType(), True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "72850273",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deserialise data from and create streaming query\n",
    "df_deser = df_read.selectExpr(\"CAST(value AS STRING)\")\\\n",
    "                   .select(from_json(\"value\", parse_schema).alias(\"value\"))\\\n",
    "                   .select(\"value.tid\", \"value.datetime\", \"value.cc_num\", \"value.amount\")\\\n",
    "                   .selectExpr(\"CAST(tid as string)\", \"CAST(datetime as string)\", \"CAST(cc_num as long)\", \"CAST(amount as double)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "1a23fc49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True"
     ]
    }
   ],
   "source": [
    "df_deser.isStreaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "42a5a973",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- tid: string (nullable = true)\n",
      " |-- datetime: string (nullable = true)\n",
      " |-- cc_num: long (nullable = true)\n",
      " |-- amount: double (nullable = true)"
     ]
    }
   ],
   "source": [
    "df_deser.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad33db75",
   "metadata": {},
   "source": [
    "## Create windowing aggregations over different time windows using spark streaming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "b4993124",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10 minute window\n",
    "windowed10mSignalDF =df_deser \\\n",
    "    .selectExpr(\"CAST(tid as string)\", \"CAST(datetime as timestamp)\", \"CAST(cc_num as long)\", \"CAST(amount as double)\")\\\n",
    "    .withWatermark(\"datetime\", \"60 minutes\") \\\n",
    "    .groupBy(window(\"datetime\", \"10 minutes\"), \"cc_num\") \\\n",
    "    .agg(avg(\"amount\").alias(\"avg_amt_per_10m\"), stddev(\"amount\").alias(\"stdev_amt_per_10m\"), count(\"cc_num\").alias(\"num_trans_per_10m\"))\\\n",
    "    .select(\"cc_num\", \"num_trans_per_10m\", \"avg_amt_per_10m\", \"stdev_amt_per_10m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "fca85c50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True"
     ]
    }
   ],
   "source": [
    "windowed10mSignalDF.isStreaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "48b5c6bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- cc_num: long (nullable = true)\n",
      " |-- num_trans_per_10m: long (nullable = false)\n",
      " |-- avg_amt_per_10m: double (nullable = true)\n",
      " |-- stdev_amt_per_10m: double (nullable = true)"
     ]
    }
   ],
   "source": [
    "windowed10mSignalDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "3877e5ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 hour window\n",
    "windowed1hSignalDF = \\\n",
    "  df_deser \\\n",
    "    .selectExpr(\"CAST(tid as string)\", \"CAST(datetime as timestamp)\", \"CAST(cc_num as long)\", \"CAST(amount as double)\")\\\n",
    "    .withWatermark(\"datetime\", \"60 minutes\") \\\n",
    "    .groupBy(window(\"datetime\", \"60 minutes\"), \"cc_num\") \\\n",
    "    .agg(avg(\"amount\").alias(\"avg_amt_per_1h\"), stddev(\"amount\").alias(\"stdev_amt_per_1h\"), count(\"cc_num\").alias(\"num_trans_per_1h\"))\\\n",
    "    .select(\"cc_num\", \"num_trans_per_1h\", \"avg_amt_per_1h\", \"stdev_amt_per_1h\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "03ebbd64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True"
     ]
    }
   ],
   "source": [
    "windowed1hSignalDF.isStreaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "8127d814",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- cc_num: long (nullable = true)\n",
      " |-- num_trans_per_1h: long (nullable = false)\n",
      " |-- avg_amt_per_1h: double (nullable = true)\n",
      " |-- stdev_amt_per_1h: double (nullable = true)"
     ]
    }
   ],
   "source": [
    "windowed1hSignalDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "188cd121",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12 hour window\n",
    "windowed12hSignalDF = \\\n",
    "  df_deser \\\n",
    "    .selectExpr(\"CAST(tid as string)\", \"CAST(datetime as timestamp)\", \"CAST(cc_num as long)\", \"CAST(amount as double)\")\\\n",
    "    .withWatermark(\"datetime\", \"60 minutes\") \\\n",
    "    .groupBy(window(\"datetime\", \"12 hours\"), \"cc_num\") \\\n",
    "    .agg(avg(\"amount\").alias(\"avg_amt_per_12h\"), stddev(\"amount\").alias(\"stdev_amt_per_12h\"), count(\"cc_num\").alias(\"num_trans_per_12h\"))\\\n",
    "    .select(\"cc_num\", \"num_trans_per_12h\", \"avg_amt_per_12h\", \"stdev_amt_per_12h\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "97e396cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True"
     ]
    }
   ],
   "source": [
    "windowed12hSignalDF.isStreaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "1397d575",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- cc_num: long (nullable = true)\n",
      " |-- num_trans_per_12h: long (nullable = false)\n",
      " |-- avg_amt_per_12h: double (nullable = true)\n",
      " |-- stdev_amt_per_12h: double (nullable = true)"
     ]
    }
   ],
   "source": [
    "windowed12hSignalDF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eae9a47",
   "metadata": {},
   "source": [
    "### Establish a connection with your Hopsworks feature store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "a4c9d870",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected. Call `.close()` to terminate connection gracefully."
     ]
    }
   ],
   "source": [
    "import hsfs\n",
    "connection = hsfs.connection()\n",
    "# get a reference to the feature store, you can access also shared feature stores by providing the feature store name\n",
    "fs = connection.get_feature_store()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d0fc58",
   "metadata": {},
   "source": [
    "## Get feature groups from hopsworks feature store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "d5d57307",
   "metadata": {},
   "outputs": [],
   "source": [
    "card_transactions = fs.get_feature_group(\"card_transactions\", version = 1)\n",
    "card_transactions_10m_agg = fs.get_feature_group(\"card_transactions_10m_agg\", version = 1)\n",
    "card_transactions_1h_agg = fs.get_feature_group(\"card_transactions_1h_agg\", version = 1)\n",
    "card_transactions_12h_agg = fs.get_feature_group(\"card_transactions_12h_agg\", version = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e99689f8",
   "metadata": {},
   "source": [
    "## Insert streaming dataframes to the online feature group\n",
    "\n",
    "Now we are ready to write this streaming dataframe as a long living application to the online storage of the other feature group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "2b277656",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_10m = card_transactions_10m_agg.insert_stream(windowed10mSignalDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "dd065552",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_1h = card_transactions_1h_agg.insert_stream(windowed1hSignalDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "3e1643b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_12h = card_transactions_12h_agg.insert_stream(windowed12hSignalDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f1a9147",
   "metadata": {},
   "source": [
    "### Check if spark streaming query is active"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "dfc53a18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True"
     ]
    }
   ],
   "source": [
    "query_10m.isActive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "b2270659",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True"
     ]
    }
   ],
   "source": [
    "query_1h.isActive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "d2b30afa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True"
     ]
    }
   ],
   "source": [
    "query_12h.isActive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b04a2b7",
   "metadata": {},
   "source": [
    "#### We can also check status of a query and if there are any exceptions trown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "15711612",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'message': 'Waiting for data to arrive', 'isDataAvailable': False, 'isTriggerActive': False}"
     ]
    }
   ],
   "source": [
    "query_10m.status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "ecf895dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'message': 'Waiting for data to arrive', 'isDataAvailable': False, 'isTriggerActive': False}"
     ]
    }
   ],
   "source": [
    "query_12h.status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "91b0fcc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_10m.exception()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "0cd6c3be",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_1h.exception()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91556489",
   "metadata": {},
   "source": [
    "### Lets check if data was ingested in to the online feature store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "3ee7f52f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------------+---------------+-----------------+\n",
      "|cc_num|num_trans_per_12h|avg_amt_per_12h|stdev_amt_per_12h|\n",
      "+------+-----------------+---------------+-----------------+\n",
      "+------+-----------------+---------------+-----------------+"
     ]
    }
   ],
   "source": [
    "fs.sql(\"SELECT * FROM card_transactions_12h_agg_1\",online=True).show(20,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "5f679242",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0"
     ]
    }
   ],
   "source": [
    "fs.sql(\"SELECT * FROM card_transactions_12h_agg_1\",online=True).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c5eed58",
   "metadata": {},
   "source": [
    "## Insert data in to offline feature group.\n",
    "Hopsworks online feature store will store latest avaible value of feature for low latency model serving. However, we also want to store data in to the offline feature store to store historical data.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "58877ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def foreach_batch_function_card(batchDF, epoch_id):\n",
    "    batchDF.persist()\n",
    "    print(epoch_id)\n",
    "    extra_hudi_options = {\n",
    "    \"hoodie.bulkinsert.shuffle.parallelism\":\"1\",     \n",
    "    \"hoodie.insert.shuffle.parallelism\":\"1\", \n",
    "    \"hoodie.upsert.shuffle.parallelism\":\"1\",\n",
    "    \"hoodie.parquet.compression.ratio\":\"0.5\"\n",
    "    }\n",
    "    # Transform and write batchDF\n",
    "    card_transactions.insert(batchDF,write_options=extra_hudi_options, storage=\"offline\")\n",
    "    batchDF.unpersist()\n",
    "\n",
    "hudi_card = df_deser.writeStream.foreachBatch(foreach_batch_function_card)\\\n",
    "                    .option(\"checkpointLocation\", hdfs.project_path() + \"/Resources/checkpoint-card\")\\\n",
    "                    .start()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "58c8339a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def foreach_batch_function_10m(batchDF, epoch_id):\n",
    "    batchDF.persist()\n",
    "    print(epoch_id)\n",
    "    extra_hudi_options = {\n",
    "    \"hoodie.bulkinsert.shuffle.parallelism\":\"1\",     \n",
    "    \"hoodie.insert.shuffle.parallelism\":\"1\", \n",
    "    \"hoodie.upsert.shuffle.parallelism\":\"1\",\n",
    "    \"hoodie.parquet.compression.ratio\":\"0.5\"\n",
    "    }\n",
    "    # Transform and write batchDF\n",
    "    card_transactions_10m_agg.insert(batchDF,write_options=extra_hudi_options, storage=\"offline\")\n",
    "    batchDF.unpersist()\n",
    "\n",
    "hudi_10m = windowed10mSignalDF.writeStream.foreachBatch(foreach_batch_function_10m)\\\n",
    "                              .option(\"checkpointLocation\", hdfs.project_path() + \"/Resources/checkpoint-data10m\")\\\n",
    "                              .start()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "62654f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def foreach_batch_function_1h(batchDF, epoch_id):\n",
    "    batchDF.persist()\n",
    "    print(epoch_id)\n",
    "    extra_hudi_options = {\n",
    "    \"hoodie.bulkinsert.shuffle.parallelism\":\"1\",     \n",
    "    \"hoodie.insert.shuffle.parallelism\":\"1\", \n",
    "    \"hoodie.upsert.shuffle.parallelism\":\"1\",\n",
    "    \"hoodie.parquet.compression.ratio\":\"0.5\"\n",
    "    }\n",
    "    # Transform and write batchDF\n",
    "    card_transactions_1h_agg.insert(batchDF,write_options=extra_hudi_options, storage=\"offline\")\n",
    "    batchDF.unpersist()\n",
    "\n",
    "hudi_1h = windowed1hSignalDF.writeStream.foreachBatch(foreach_batch_function_1h)\\\n",
    "                            .option(\"checkpointLocation\", hdfs.project_path() + \"/Resources/checkpoint-1h\")\\\n",
    "                            .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "05a4d545",
   "metadata": {},
   "outputs": [],
   "source": [
    "def foreach_batch_function_12h(batchDF, epoch_id):\n",
    "    batchDF.persist()\n",
    "    print(epoch_id)\n",
    "    extra_hudi_options = {\n",
    "    \"hoodie.bulkinsert.shuffle.parallelism\":\"1\",     \n",
    "    \"hoodie.insert.shuffle.parallelism\":\"1\", \n",
    "    \"hoodie.upsert.shuffle.parallelism\":\"1\",\n",
    "    \"hoodie.parquet.compression.ratio\":\"0.5\"\n",
    "    }\n",
    "    # Transform and write batchDF\n",
    "    card_transactions_12h_agg.insert(batchDF,write_options=extra_hudi_options, storage=\"offline\")\n",
    "    batchDF.unpersist()\n",
    "\n",
    "hudi_12h = windowed12hSignalDF.writeStream.foreachBatch(foreach_batch_function_12h)\\\n",
    "                              .option(\"checkpointLocation\", hdfs.project_path() + \"/Resources/checkpoint-12h\")\\\n",
    "                              .start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1940a2be",
   "metadata": {},
   "source": [
    "### Check if queries are still active"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "ba7ea282",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False"
     ]
    }
   ],
   "source": [
    "hudi_card.isActive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "6401b497",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False"
     ]
    }
   ],
   "source": [
    "hudi_10m.isActive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "9669b9c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False"
     ]
    }
   ],
   "source": [
    "hudi_1h.isActive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "e5d31bd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False"
     ]
    }
   ],
   "source": [
    "hudi_12h.isActive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef1f3e7",
   "metadata": {},
   "source": [
    "### Stop queries\n",
    "If you are running this from a notebook, you can kill the Spark Structured Streaming Query by stopping the Kernel or by calling its `.stop()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "498584d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_10m.stop()\n",
    "query_1h.stop()\n",
    "query_12h.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "6de6fbc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "hudi_card.stop()\n",
    "hudi_10m.stop()\n",
    "hudi_1h.stop()\n",
    "hudi_12h.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "887bba49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e3f5a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}